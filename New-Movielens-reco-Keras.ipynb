{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1458635171.0\n",
      "0.7999920663255187\n",
      "0.19999801658137967\n",
      "(80668, 3)\n",
      "(20168, 3)\n",
      "Train on 64534 samples, validate on 16134 samples\n",
      "Epoch 1/10\n",
      "64534/64534 [==============================] - 6s 92us/step - loss: 2.2548 - mean_absolute_error: 2.2548 - val_loss: 1.9055 - val_mean_absolute_error: 1.9055\n",
      "Epoch 2/10\n",
      "64534/64534 [==============================] - 5s 82us/step - loss: 0.8411 - mean_absolute_error: 0.8411 - val_loss: 1.4004 - val_mean_absolute_error: 1.4004\n",
      "Epoch 3/10\n",
      "64534/64534 [==============================] - 5s 82us/step - loss: 0.7042 - mean_absolute_error: 0.7042 - val_loss: 1.1890 - val_mean_absolute_error: 1.1890\n",
      "Epoch 4/10\n",
      "64534/64534 [==============================] - 5s 82us/step - loss: 0.6611 - mean_absolute_error: 0.6611 - val_loss: 1.1016 - val_mean_absolute_error: 1.1016\n",
      "Epoch 5/10\n",
      "64534/64534 [==============================] - 5s 81us/step - loss: 0.6369 - mean_absolute_error: 0.6369 - val_loss: 1.0674 - val_mean_absolute_error: 1.0674\n",
      "Epoch 6/10\n",
      "64534/64534 [==============================] - 5s 82us/step - loss: 0.6144 - mean_absolute_error: 0.6144 - val_loss: 1.0380 - val_mean_absolute_error: 1.0380\n",
      "Epoch 7/10\n",
      "64534/64534 [==============================] - 5s 81us/step - loss: 0.5990 - mean_absolute_error: 0.5990 - val_loss: 1.0063 - val_mean_absolute_error: 1.0063\n",
      "Epoch 8/10\n",
      "64534/64534 [==============================] - 5s 81us/step - loss: 0.5814 - mean_absolute_error: 0.5814 - val_loss: 0.9880 - val_mean_absolute_error: 0.9880\n",
      "Epoch 9/10\n",
      "64534/64534 [==============================] - 5s 80us/step - loss: 0.5688 - mean_absolute_error: 0.5688 - val_loss: 0.9675 - val_mean_absolute_error: 0.9675\n",
      "Epoch 10/10\n",
      "64534/64534 [==============================] - 5s 82us/step - loss: 0.5582 - mean_absolute_error: 0.5582 - val_loss: 0.9519 - val_mean_absolute_error: 0.9519\n",
      " Test Mae model 1 : 0.9483242885419084 \n",
      "Train on 64534 samples, validate on 16134 samples\n",
      "Epoch 1/10\n",
      "64534/64534 [==============================] - 7s 102us/step - loss: 2.0378 - mean_absolute_error: 2.0378 - val_loss: 1.8088 - val_mean_absolute_error: 1.8088\n",
      "Epoch 2/10\n",
      "64534/64534 [==============================] - 6s 90us/step - loss: 0.8772 - mean_absolute_error: 0.8772 - val_loss: 1.3228 - val_mean_absolute_error: 1.3228\n",
      "Epoch 3/10\n",
      "64534/64534 [==============================] - 6s 90us/step - loss: 0.7073 - mean_absolute_error: 0.7073 - val_loss: 1.1111 - val_mean_absolute_error: 1.1111\n",
      "Epoch 4/10\n",
      "64534/64534 [==============================] - 6s 90us/step - loss: 0.6503 - mean_absolute_error: 0.6503 - val_loss: 1.0460 - val_mean_absolute_error: 1.0460\n",
      "Epoch 5/10\n",
      "64534/64534 [==============================] - 6s 90us/step - loss: 0.6203 - mean_absolute_error: 0.6203 - val_loss: 0.9961 - val_mean_absolute_error: 0.9961\n",
      "Epoch 6/10\n",
      "64534/64534 [==============================] - 6s 90us/step - loss: 0.5949 - mean_absolute_error: 0.5949 - val_loss: 0.9829 - val_mean_absolute_error: 0.9829\n",
      "Epoch 7/10\n",
      "64534/64534 [==============================] - 6s 91us/step - loss: 0.5802 - mean_absolute_error: 0.5802 - val_loss: 0.9559 - val_mean_absolute_error: 0.9559\n",
      "Epoch 8/10\n",
      "64534/64534 [==============================] - 6s 90us/step - loss: 0.5615 - mean_absolute_error: 0.5615 - val_loss: 0.9487 - val_mean_absolute_error: 0.9487\n",
      "Epoch 9/10\n",
      "64534/64534 [==============================] - 6s 90us/step - loss: 0.5514 - mean_absolute_error: 0.5514 - val_loss: 0.9335 - val_mean_absolute_error: 0.9335\n",
      "Epoch 10/10\n",
      "64534/64534 [==============================] - 6s 90us/step - loss: 0.5411 - mean_absolute_error: 0.5411 - val_loss: 0.9267 - val_mean_absolute_error: 0.9267\n",
      " Test Mae model 2 : 0.927058987476106 \n",
      "Train on 64534 samples, validate on 16134 samples\n",
      "Epoch 1/10\n",
      "64534/64534 [==============================] - 7s 110us/step - loss: 1.0223 - mean_absolute_error: 1.0223 - val_loss: 0.9038 - val_mean_absolute_error: 0.9038\n",
      "Epoch 2/10\n",
      "64534/64534 [==============================] - 6s 98us/step - loss: 0.6292 - mean_absolute_error: 0.6292 - val_loss: 0.9180 - val_mean_absolute_error: 0.9180\n",
      "Epoch 3/10\n",
      "64534/64534 [==============================] - 6s 97us/step - loss: 0.5572 - mean_absolute_error: 0.5572 - val_loss: 0.8848 - val_mean_absolute_error: 0.8848\n",
      "Epoch 4/10\n",
      "64534/64534 [==============================] - 6s 97us/step - loss: 0.5181 - mean_absolute_error: 0.5181 - val_loss: 0.8744 - val_mean_absolute_error: 0.8744\n",
      "Epoch 5/10\n",
      "64534/64534 [==============================] - 6s 98us/step - loss: 0.4937 - mean_absolute_error: 0.4937 - val_loss: 0.8755 - val_mean_absolute_error: 0.8755\n",
      "Epoch 6/10\n",
      "64534/64534 [==============================] - 6s 99us/step - loss: 0.4781 - mean_absolute_error: 0.4781 - val_loss: 0.8707 - val_mean_absolute_error: 0.8707\n",
      "Epoch 7/10\n",
      "64534/64534 [==============================] - 6s 97us/step - loss: 0.4649 - mean_absolute_error: 0.4649 - val_loss: 0.8444 - val_mean_absolute_error: 0.8444\n",
      "Epoch 8/10\n",
      "64534/64534 [==============================] - 6s 97us/step - loss: 0.4556 - mean_absolute_error: 0.4556 - val_loss: 0.8621 - val_mean_absolute_error: 0.8621\n",
      "Epoch 9/10\n",
      "64534/64534 [==============================] - 6s 97us/step - loss: 0.4486 - mean_absolute_error: 0.4486 - val_loss: 0.8360 - val_mean_absolute_error: 0.8360\n",
      "Epoch 10/10\n",
      "64534/64534 [==============================] - 6s 99us/step - loss: 0.4424 - mean_absolute_error: 0.4424 - val_loss: 0.8418 - val_mean_absolute_error: 0.8418\n",
      " Test Mae model 3 : 0.8370298216239841 \n",
      "1458635171.0\n",
      "0.7999920663255187\n",
      "0.19999801658137967\n",
      "(80668, 3)\n",
      "(20168, 3)\n",
      "[[ 0.04973532  0.04982212  0.00041407 ...  0.03007351 -0.00178476\n",
      "  -0.04186865]\n",
      " [ 0.12134685  0.16361068  0.10502332 ...  0.18737651 -0.11195848\n",
      "  -0.23810238]\n",
      " [-0.06399585  0.15757044  0.29366466 ... -0.02647739  0.03571346\n",
      "   0.05992419]\n",
      " ...\n",
      " [ 0.06736252  0.164871   -0.19363971 ...  0.20908405 -0.2265547\n",
      "  -0.09736341]\n",
      " [-0.01773924  0.01106733  0.04448141 ... -0.01305462  0.00543001\n",
      "  -0.03663677]\n",
      " [-0.04775508  0.00361674  0.03368929 ... -0.06210157  0.04705177\n",
      "   0.09316587]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'movies.html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source : https://github.com/CVxTz/Recommender_keras\n",
    "# dataset : https://grouplens.org/datasets/movielens/    \n",
    "# dataset file :  ml-latest-small.zip\n",
    "# dataset readme : http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "\n",
    "# This code is a combination of utils.py, recommend.py, and plot_movies.py. \n",
    "# And, it also runs and validates in python 3.5 and keras 2.0.   \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense, Flatten, Dropout\n",
    "from keras.layers.merge import Dot, multiply, concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from collections import defaultdict\n",
    "### --- \n",
    "# from utils import *  # utils.py\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "### --- \n",
    "#from utils import *  # utils.py\n",
    "#import pickle\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.plotly as py  # conda install -c plotly plotly\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "from keras.models import load_model  # NEW \n",
    "\n",
    "\n",
    "\n",
    "### --- start of  Recommender_keras/utils.py\n",
    "def get_mapping(series):\n",
    "    occurances = defaultdict(int)\n",
    "    for element in series:\n",
    "        occurances[element] += 1\n",
    "    mapping = {}\n",
    "    i = 0\n",
    "    for element in occurances:\n",
    "        i += 1\n",
    "        mapping[element] = i\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def get_data():\n",
    "    #data = pd.read_csv(\"data/ratings.csv\")\n",
    "    data = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "    \n",
    "    mapping_work = get_mapping(data[\"movieId\"])\n",
    "\n",
    "    data[\"movieId\"] = data[\"movieId\"].map(mapping_work)\n",
    "\n",
    "    mapping_users = get_mapping(data[\"movieId\"])\n",
    "\n",
    "    data[\"movieId\"] = data[\"movieId\"].map(mapping_users)\n",
    "\n",
    "    percentil_80 = np.percentile(data[\"timestamp\"], 80)\n",
    "\n",
    "    print(percentil_80)\n",
    "    print(np.mean(data[\"timestamp\"]<percentil_80))\n",
    "    print(np.mean(data[\"timestamp\"]>percentil_80))\n",
    "\n",
    "    cols = [\"userId\", \"movieId\", \"rating\"]\n",
    "\n",
    "    train = data[data.timestamp<percentil_80][cols]\n",
    "\n",
    "    print(train.shape) #(80668, 3)\n",
    "\n",
    "    test = data[data.timestamp>=percentil_80][cols]\n",
    "\n",
    "    print(test.shape) #(20168, 3)\n",
    "\n",
    "    max_user = max(data[\"userId\"].tolist() )\n",
    "    max_work = max(data[\"movieId\"].tolist() )\n",
    "    return train, test, max_user, max_work, mapping_work\n",
    "\n",
    "\n",
    "def get_model_1(max_work, max_user):\n",
    "    dim_embedddings = 30\n",
    "    bias = 3\n",
    "    # inputs\n",
    "    w_inputs = Input(shape=(1,), dtype='int32')\n",
    "    w = Embedding(max_work+1, dim_embedddings, name=\"work\")(w_inputs)\n",
    "\n",
    "    # context\n",
    "    u_inputs = Input(shape=(1,), dtype='int32')\n",
    "    u = Embedding(max_user+1, dim_embedddings, name=\"user\")(u_inputs)\n",
    "    o = multiply([w, u])\n",
    "    o = Dropout(0.5)(o)\n",
    "    o = Flatten()(o)\n",
    "    o = Dense(1)(o)\n",
    "\n",
    "    rec_model = Model(inputs=[w_inputs, u_inputs], outputs=o)\n",
    "    #rec_model.summary()\n",
    "    rec_model.compile(loss='mae', optimizer='adam', metrics=[\"mae\"])\n",
    "    return rec_model\n",
    "\n",
    "def get_model_2(max_work, max_user):\n",
    "    dim_embedddings = 30\n",
    "    bias = 1\n",
    "    # inputs\n",
    "    w_inputs = Input(shape=(1,), dtype='int32')\n",
    "    w = Embedding(max_work+1, dim_embedddings, name=\"work\")(w_inputs)\n",
    "    w_bis = Embedding(max_work + 1, bias, name=\"workbias\")(w_inputs)\n",
    "\n",
    "    # context\n",
    "    u_inputs = Input(shape=(1,), dtype='int32')\n",
    "    u = Embedding(max_user+1, dim_embedddings, name=\"user\")(u_inputs)\n",
    "    u_bis = Embedding(max_user + 1, bias, name=\"userbias\")(u_inputs)\n",
    "    o = multiply([w, u])\n",
    "    o = concatenate([o, u_bis, w_bis])\n",
    "    o = Dropout(0.5)(o)\n",
    "    o = Flatten()(o)\n",
    "    o = Dense(1)(o)\n",
    "\n",
    "    rec_model = Model(inputs=[w_inputs, u_inputs], outputs=o)\n",
    "    #rec_model.summary()\n",
    "    rec_model.compile(loss='mae', optimizer='adam', metrics=[\"mae\"])\n",
    "    return rec_model\n",
    "\n",
    "def get_model_3(max_work, max_user):\n",
    "    dim_embedddings = 30\n",
    "    bias = 1\n",
    "    # inputs\n",
    "    w_inputs = Input(shape=(1,), dtype='int32')\n",
    "    w = Embedding(max_work+1, dim_embedddings, name=\"work\")(w_inputs)\n",
    "    w_bis = Embedding(max_work + 1, bias, name=\"workbias\")(w_inputs)\n",
    "\n",
    "    # context\n",
    "    u_inputs = Input(shape=(1,), dtype='int32')\n",
    "    u = Embedding(max_user+1, dim_embedddings, name=\"user\")(u_inputs)\n",
    "    u_bis = Embedding(max_user + 1, bias, name=\"userbias\")(u_inputs)\n",
    "    o = multiply([w, u])\n",
    "    o = Dropout(0.5)(o)\n",
    "    o = concatenate([o, u_bis, w_bis])\n",
    "    o = Flatten()(o)\n",
    "    o = Dense(10, activation=\"relu\")(o)\n",
    "    o = Dense(1)(o)\n",
    "\n",
    "    rec_model = Model(inputs=[w_inputs, u_inputs], outputs=o)\n",
    "    #rec_model.summary()\n",
    "    rec_model.compile(loss='mae', optimizer='adam', metrics=[\"mae\"])\n",
    "    return rec_model\n",
    "\n",
    "def get_array(series):\n",
    "    return np.array([[element] for element in series])\n",
    "### --- end of  Recommender_keras/utils.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### --- start of Recommender_keras/recommend.py\n",
    "train, test, max_user, max_work, mapping_work = get_data()\n",
    "\n",
    "\n",
    "pickle.dump(mapping_work, open('mapping_work.pkl', 'wb')) # create pkl file\n",
    "\n",
    "####################\n",
    "model = get_model_1(max_work, max_user)\n",
    "\n",
    "history = model.fit([get_array(train[\"movieId\"]), get_array(train[\"userId\"])], get_array(train[\"rating\"]), epochs=10,\n",
    "                    validation_split=0.2, verbose=1)\n",
    "\n",
    "model.save_weights(\"model_1.h5\")  # create hdf5 file\n",
    "\n",
    "predictions = model.predict([get_array(test[\"movieId\"]), get_array(test[\"userId\"])])\n",
    "\n",
    "test_performance = mean_absolute_error(test[\"rating\"], predictions)\n",
    "\n",
    "print(\" Test Mae model 1 : %s \" % test_performance)\n",
    "\n",
    "\n",
    "####################\n",
    "model = get_model_2(max_work, max_user)\n",
    "\n",
    "history = model.fit([get_array(train[\"movieId\"]), get_array(train[\"userId\"])], get_array(train[\"rating\"]), epochs=10,\n",
    "                    validation_split=0.2, verbose=1)\n",
    "\n",
    "predictions = model.predict([get_array(test[\"movieId\"]), get_array(test[\"userId\"])])\n",
    "\n",
    "test_performance = mean_absolute_error(test[\"rating\"], predictions)\n",
    "\n",
    "print(\" Test Mae model 2 : %s \" % test_performance)\n",
    "\n",
    "####################\n",
    "model = get_model_3(max_work, max_user)\n",
    "\n",
    "history = model.fit([get_array(train[\"movieId\"]), get_array(train[\"userId\"])], get_array(train[\"rating\"]), epochs=10,\n",
    "                    validation_split=0.2, verbose=1)\n",
    "\n",
    "predictions = model.predict([get_array(test[\"movieId\"]), get_array(test[\"userId\"])])\n",
    "\n",
    "test_performance = mean_absolute_error(test[\"rating\"], predictions)\n",
    "\n",
    "print(\" Test Mae model 3 : %s \" % test_performance)\n",
    "### --- End of Recommender_keras/recommend.py\n",
    "\n",
    "\"\"\"\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "\"\"\"\n",
    "\n",
    "### --- start of Recommender_keras/plot_movies.py\n",
    "train, test, max_user, max_work, _ = get_data()\n",
    "\n",
    "#movies = pd.read_csv(\"data/movies.csv\")\n",
    "movies = pd.read_csv(\"ml-latest-small/movies.csv\")\n",
    "    \n",
    "movie_title = dict(zip(movies[\"movieId\"], movies[\"title\"]))\n",
    "\n",
    "model = get_model_1(max_user=max_user, max_work=max_work)\n",
    "\n",
    "\n",
    "model.load_weights(\"model_1.h5\")   # load hdf5 \n",
    "\n",
    "\n",
    "embedding_work = model.get_layer(\"work\").get_weights()[0]\n",
    "\n",
    "print(embedding_work)\n",
    "\n",
    "mapping_work = pickle.load(open(\"mapping_work.pkl\", \"rb\"))  # load pkl (pickle) file\n",
    "\n",
    "# reverse_mapping = dict((v,k) for k,v in mapping_work.iteritems())\n",
    "# Error: “ 'dict' object has no attribute 'iteritems' ”\n",
    "# Removed dict.iteritems(), dict.iterkeys(), and dict.itervalues()\n",
    "# Instead: use dict.items(), dict.keys(), and dict.values() respectively.\n",
    "\n",
    "reverse_mapping = dict((v,k) for k,v in mapping_work.items())  # iteritems -> items\n",
    "\n",
    "embedding = {}\n",
    "\n",
    "for id in movie_title:\n",
    "    if id in mapping_work:\n",
    "        embedding[id] = embedding_work[mapping_work[id], :]\n",
    "\n",
    "\n",
    "list_titles = []\n",
    "list_embeddings = []\n",
    "\n",
    "for id in embedding:\n",
    "    list_titles.append(movie_title[id])\n",
    "    list_embeddings.append(embedding[id])\n",
    "\n",
    "matrix_embedding = np.array(list_embeddings)\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(matrix_embedding)\n",
    "\n",
    "vis_x = X_embedded[:, 0]\n",
    "vis_y = X_embedded[:, 1]\n",
    "\n",
    "\n",
    "data = [\n",
    "    go.Scatter(\n",
    "        x=vis_x,\n",
    "        y=vis_y,\n",
    "        mode='markers',\n",
    "        text=list_titles\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Movies'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "plotly.offline.plot(fig, filename='movies.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
